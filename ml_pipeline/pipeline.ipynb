{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "285368df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, rand\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.sql.types import DoubleType\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0c0c79de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âTAPE 1 : R√©cup√©ration des donn√©es depuis MongoDB\n",
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"clients_bancaires\"]\n",
    "collection = db[\"df_clean_collection\"]\n",
    "\n",
    "# exemple r√©cup√©rer seulement 5 documents sans le champ _id\n",
    "docs = list(collection.find({}, {\"_id\": 0}).limit(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "56316da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# R√©cup√©rer les documents sans le champ _id\n",
    "data = list(collection.find({}, {\"_id\": 0}))\n",
    "df_pandas = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "124de4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyer les donn√©es dans Pandas\n",
    "df_pandas = df_pandas.drop([ 'CustomerId', 'Surname'], axis=1, errors='ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ac06c015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire GeographyVec\n",
    "def extract_geography(vec):\n",
    "    france = 0.0\n",
    "    germany = 0.0\n",
    "    if vec is not None and len(vec) == 3:\n",
    "        # vec = (taille, indices, valeurs)\n",
    "        indices = vec[1]\n",
    "        values = vec[2]\n",
    "        for idx, val in zip(indices, values):\n",
    "            if idx == 0:\n",
    "                france = float(val)\n",
    "            elif idx == 1:\n",
    "                germany = float(val)\n",
    "    return pd.Series([france, germany])\n",
    "\n",
    "df_pandas[['Geography_France', 'Geography_Germany']] = df_pandas['GeographyVec'].apply(extract_geography)\n",
    "df_pandas = df_pandas.drop('GeographyVec', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "aed3b556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "      <th>GenderIndex</th>\n",
       "      <th>Geography_France</th>\n",
       "      <th>Geography_Germany</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>850</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>645</td>\n",
       "      <td>44</td>\n",
       "      <td>8</td>\n",
       "      <td>113755.78</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>149756.71</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>822</td>\n",
       "      <td>50</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10062.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>376</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>115046.74</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>119346.88</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>501</td>\n",
       "      <td>44</td>\n",
       "      <td>4</td>\n",
       "      <td>142051.07</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>74940.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>684</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>134603.88</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>71725.73</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CreditScore Age Tenure    Balance NumOfProducts HasCrCard IsActiveMember  \\\n",
       "0         619  42      2          0             1         1              1   \n",
       "1         608  41      1   83807.86             1         0              1   \n",
       "2         502  42      8   159660.8             3         1              0   \n",
       "3         699  39      1          0             2         0              0   \n",
       "4         850  43      2  125510.82             1         1              1   \n",
       "5         645  44      8  113755.78             2         1              0   \n",
       "6         822  50      7          0             2         1              1   \n",
       "7         376  29      4  115046.74             4         1              0   \n",
       "8         501  44      4  142051.07             2         0              1   \n",
       "9         684  27      2  134603.88             1         1              1   \n",
       "\n",
       "  EstimatedSalary Exited  GenderIndex  Geography_France  Geography_Germany  \n",
       "0       101348.88      1          1.0               0.0                0.0  \n",
       "1       112542.58      0          1.0               0.0                0.0  \n",
       "2       113931.57      1          1.0               0.0                0.0  \n",
       "3        93826.63      0          1.0               0.0                0.0  \n",
       "4         79084.1      0          1.0               0.0                0.0  \n",
       "5       149756.71      1          0.0               0.0                0.0  \n",
       "6         10062.8      0          0.0               0.0                0.0  \n",
       "7       119346.88      1          1.0               0.0                0.0  \n",
       "8         74940.5      0          0.0               0.0                0.0  \n",
       "9        71725.73      0          0.0               0.0                0.0  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4f88ebfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession is running...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# üö®üö®üö® ÿ£ŸàŸÑ ÿ≠ÿßÿ¨ÿ©: ŸÑÿßÿ≤ŸÖ ÿ™ÿπŸàÿ∂ ŸáÿßÿØ ÿßŸÑÿ≥ÿ∑ÿ± ÿ®ÿßŸÑŸÖÿ≥ÿßÿ± ÿßŸÑÿµÿ≠Ÿäÿ≠ ÿØŸäÿßŸÑ ÿßŸÑŸÄ venv ÿßŸÑÿ¨ÿØŸäÿØ üö®üö®üö®\n",
    "# (ŸÖÿ´ŸÑÿß: C:\\Users\\elkho\\SparkProjects\\breif6_...\\venv\\Scripts\\python.exe)\n",
    "NEW_PYTHON_EXECUTABLE = r'C:\\Users\\elkho\\SparkProjects\\breif6_PredictiondelAttritionClientBancaire\\venv\\Scripts\\python.exe'\n",
    "\n",
    "# 1. ŸÉŸÜÿ≠ÿØÿØŸàÿß ŸÑŸÄ Spark ÿ®ÿßŸÑÿ∂ÿ®ÿ∑ ÿßŸÑÿ®ÿßŸäÿ´ŸàŸÜ ÿßŸÑŸÑŸä ÿÆÿßÿµŸà ŸäÿÆÿØŸÖ ÿ®ŸäŸá\n",
    "os.environ['PYSPARK_PYTHON'] = NEW_PYTHON_EXECUTABLE\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = NEW_PYTHON_EXECUTABLE\n",
    "\n",
    "# 2. ŸÉŸÜÿ≤ŸäÿØŸà ÿßŸÑŸÉŸàŸÜŸÅŸäÿ∫Ÿàÿ±ÿßÿ≥ŸäŸàŸÜ ÿØŸäÿßŸÑ ÿßŸÑÿ¥ÿ®ŸÉÿ© Ÿà ÿßŸÑŸÄ Timeout\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-Dfile.encoding=UTF-8\")\\\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.python.worker.timeout\", \"600\") \\\n",
    "    .config(\"spark.driver.port\", \"50000\") \\\n",
    "    .config(\"spark.blockManager.port\", \"50001\") \\\n",
    "    .appName(\"MonProjetSpark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"SparkSession is running...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78a5433d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avant de cr√©er une nouvelle session, arr√™ter l'ancienne\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Arr√™ter toute session existante\n",
    "SparkSession.builder.getOrCreate().stop()\n",
    "\n",
    "# Ou forcer l'arr√™t\n",
    "import time\n",
    "try:\n",
    "    spark.stop()\n",
    "    time.sleep(2)  # Attendre que les ports se lib√®rent\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Cr√©er une nouvelle session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Bancaire pipeline\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e630a419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Cr√©er Spark Session\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Bancaire pipeline\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1eae2141",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas_sample = df_pandas.head(5000)\n",
    "df_pandas.to_csv('../data/df_pandas.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81837ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+------+---------+-------------+---------+--------------+---------------+------+-----------+----------------+-----------------+\n",
      "|CreditScore|Age|Tenure|  Balance|NumOfProducts|HasCrCard|IsActiveMember|EstimatedSalary|Exited|GenderIndex|Geography_France|Geography_Germany|\n",
      "+-----------+---+------+---------+-------------+---------+--------------+---------------+------+-----------+----------------+-----------------+\n",
      "|        619| 42|     2|      0.0|            1|        1|             1|      101348.88|     1|        1.0|             0.0|              0.0|\n",
      "|        608| 41|     1| 83807.86|            1|        0|             1|      112542.58|     0|        1.0|             0.0|              0.0|\n",
      "|        502| 42|     8| 159660.8|            3|        1|             0|      113931.57|     1|        1.0|             0.0|              0.0|\n",
      "|        699| 39|     1|      0.0|            2|        0|             0|       93826.63|     0|        1.0|             0.0|              0.0|\n",
      "|        850| 43|     2|125510.82|            1|        1|             1|        79084.1|     0|        1.0|             0.0|              0.0|\n",
      "+-----------+---+------+---------+-------------+---------+--------------+---------------+------+-----------+----------------+-----------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df_spark = spark.read.csv('../data/df_pandas.csv', header=True, inferSchema=True)\n",
    "df_spark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "137d9805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir les colonnes en types num√©riques appropri√©s\n",
    "numeric_columns = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', \n",
    "                   'HasCrCard', 'IsActiveMember', 'EstimatedSalary', 'Exited', 'GenderIndex']\n",
    "for col_name in numeric_columns:\n",
    "    df_spark = df_spark.withColumn(col_name, col(col_name).cast(DoubleType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "423a7b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CreditScore: double (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Tenure: double (nullable = true)\n",
      " |-- Balance: double (nullable = true)\n",
      " |-- NumOfProducts: double (nullable = true)\n",
      " |-- HasCrCard: double (nullable = true)\n",
      " |-- IsActiveMember: double (nullable = true)\n",
      " |-- EstimatedSalary: double (nullable = true)\n",
      " |-- Exited: double (nullable = true)\n",
      " |-- GenderIndex: double (nullable = true)\n",
      " |-- Geography_France: double (nullable = true)\n",
      " |-- Geography_Germany: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9140ca34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donn√©es r√©cup√©r√©es depuis MongoDB:\n",
      "Nombre total de lignes: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Donn√©es r√©cup√©r√©es depuis MongoDB:\")\n",
    "print(f\"Nombre total de lignes: {df_spark.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efc03193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|Exited|count|\n",
      "+------+-----+\n",
      "|   0.0| 7963|\n",
      "|   1.0| 2037|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# V√©rifier et g√©rer le d√©s√©quilibre de classes\n",
    "# Compter combien de clients ont Exited=1 et Exited=0\n",
    "\n",
    "df_spark.groupBy(\"Exited\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fe19e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# le dataset est d√©s√©quilibr√©, on peut appliquer un sur√©chantillonnage de la classe minoritaire\n",
    "nombre_exited_1 = df_spark.filter(col(\"Exited\") == 1).count()\n",
    "nombre_exited_0 = df_spark.filter(col(\"Exited\") == 0).count()\n",
    "\n",
    "min_count = min(nombre_exited_0, nombre_exited_1)\n",
    "\n",
    "df_classe_0 = df_spark.filter(col(\"Exited\") == 0).sample(False, min_count / nombre_exited_0, seed=42)\n",
    "df_classe_1 = df_spark.filter(col(\"Exited\") == 1).sample(False, min_count / nombre_exited_1, seed=42)\n",
    "\n",
    "df_equilibre = df_classe_0.union(df_classe_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98cb421b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|Exited|count|\n",
      "+------+-----+\n",
      "|   0.0| 2119|\n",
      "|   1.0| 2037|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# verifierer l'√©quilibre\n",
    "df_equilibre.groupBy(\"Exited\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c67df80",
   "metadata": {},
   "source": [
    "Ici, j‚Äôai perdu beaucoup d‚Äôinformations avec l‚Äôundersampling,\n",
    "alors je passe √† une autre m√©thode : j‚Äôai appliqu√© under et over en m√™me temps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1bdaacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|Exited|count|\n",
      "+------+-----+\n",
      "|   0.0| 4062|\n",
      "|   1.0| 4019|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Comptage du nombre d'observations dans chaque classe\n",
    "nombre_restants = df_spark.filter(col(\"Exited\") == 0).count()  # Clients rest√©s\n",
    "nombre_sortis = df_spark.filter(col(\"Exited\") == 1).count()    # Clients sortis\n",
    "\n",
    "# D√©finition du ratio cible d'√©quilibrage (ex : 0.4 = 40% de chaque classe)\n",
    "ratio_cible = 0.4\n",
    "total_cible = nombre_restants + nombre_sortis\n",
    "cible_sortis = int(total_cible * ratio_cible)\n",
    "cible_restants = cible_sortis  # √©quilibre parfait\n",
    "\n",
    "# √âchantillonnage des deux classes selon le ratio\n",
    "classe_restants = df_spark.filter(col(\"Exited\") == 0).sample(False, cible_restants / nombre_restants, seed=42)\n",
    "classe_sortis = df_spark.filter(col(\"Exited\") == 1).sample(True, cible_sortis / nombre_sortis, seed=42)\n",
    "\n",
    "# Union des deux classes pour cr√©er un DataFrame √©quilibr√©\n",
    "df_equilibre = classe_restants.union(classe_sortis)\n",
    "\n",
    "# V√©rification de l'√©quilibre obtenu\n",
    "df_equilibre.groupBy(\"Exited\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "565cc848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features assembl√©es (premiers exemples):\n",
      "+-----------+----+------+-------+-------------+---------+--------------+---------------+-----------+----------------+-----------------+------------------------------------------------------+\n",
      "|CreditScore|Age |Tenure|Balance|NumOfProducts|HasCrCard|IsActiveMember|EstimatedSalary|GenderIndex|Geography_France|Geography_Germany|features_raw                                          |\n",
      "+-----------+----+------+-------+-------------+---------+--------------+---------------+-----------+----------------+-----------------+------------------------------------------------------+\n",
      "|822.0      |50.0|7.0   |0.0    |2.0          |1.0      |1.0           |10062.8        |0.0        |0.0             |0.0              |[822.0,50.0,7.0,0.0,2.0,1.0,1.0,10062.8,0.0,0.0,0.0]  |\n",
      "|497.0      |24.0|3.0   |0.0    |2.0          |1.0      |0.0           |76390.01       |0.0        |0.0             |0.0              |(11,[0,1,2,4,5,7],[497.0,24.0,3.0,2.0,1.0,76390.01])  |\n",
      "|635.0      |35.0|7.0   |0.0    |2.0          |1.0      |1.0           |65951.65       |1.0        |0.0             |0.0              |[635.0,35.0,7.0,0.0,2.0,1.0,1.0,65951.65,1.0,0.0,0.0] |\n",
      "|549.0      |24.0|9.0   |0.0    |2.0          |1.0      |1.0           |14406.41       |1.0        |0.0             |0.0              |[549.0,24.0,9.0,0.0,2.0,1.0,1.0,14406.41,1.0,0.0,0.0] |\n",
      "|636.0      |32.0|8.0   |0.0    |2.0          |1.0      |0.0           |138555.46      |1.0        |0.0             |0.0              |[636.0,32.0,8.0,0.0,2.0,1.0,0.0,138555.46,1.0,0.0,0.0]|\n",
      "+-----------+----+------+-------+-------------+---------+--------------+---------------+-----------+----------------+-----------------+------------------------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# √âTAPE 6 : D√©finir les colonnes features\n",
    "# ============================================# Toutes les colonnes num√©riques (features)\n",
    "colonnes_features = [\n",
    "    \"CreditScore\",\n",
    "    \"Age\",\n",
    "    \"Tenure\",\n",
    "    \"Balance\",\n",
    "    \"NumOfProducts\",\n",
    "    \"HasCrCard\",\n",
    "    \"IsActiveMember\",\n",
    "    \"EstimatedSalary\",\n",
    "    \"GenderIndex\",    \n",
    "    \"Geography_France\",\n",
    "    \"Geography_Germany\" \n",
    "]\n",
    "\n",
    "# Colonne cible\n",
    "colonne_cible = \"Exited\"\n",
    "\n",
    "#Assembler toutes les features dans un vecteur\n",
    "\n",
    "\n",
    "# Cr√©er le VectorAssembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=colonnes_features,\n",
    "    outputCol=\"features_raw\"\n",
    ")\n",
    "\n",
    "# Appliquer l'assembler\n",
    "df_avec_features = assembler.transform(df_equilibre)\n",
    "\n",
    "# Afficher le r√©sultat\n",
    "print(\"Features assembl√©es (premiers exemples):\")\n",
    "df_avec_features.select(colonnes_features + [\"features_raw\"]).show(5, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7187d0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features normalis√©es (premiers exemples):\n",
      "+------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|features_raw                                          |features                                                                                                                                                                                  |\n",
      "+------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[822.0,50.0,7.0,0.0,2.0,1.0,1.0,10062.8,0.0,0.0,0.0]  |[1.7554119083328914,0.8301437864465926,0.6765702918234291,-1.3500498587716774,0.7531503687480621,0.6568705569879226,1.097616608426526,-1.581212567159642,-1.0020439117270565,0.0,0.0]     |\n",
      "|(11,[0,1,2,4,5,7],[497.0,24.0,3.0,2.0,1.0,76390.01])  |[-1.546025367949272,-1.620995795823093,-0.7037287559486252,-1.3500498587716774,0.7531503687480621,0.6568705569879226,-0.9109521897380475,-0.43051687793206256,-1.0020439117270565,0.0,0.0]|\n",
      "|[635.0,35.0,7.0,0.0,2.0,1.0,1.0,65951.65,1.0,0.0,0.0] |[-0.14418430909715332,-0.5839752033243798,0.6765702918234291,-1.3500498587716774,0.7531503687480621,0.6568705569879226,1.097616608426526,-0.611609612793149,0.9978367626780669,0.0,0.0]   |\n",
      "|[549.0,24.0,9.0,0.0,2.0,1.0,1.0,14406.41,1.0,0.0,0.0] |[-1.0177954037441257,-1.620995795823093,1.3667198157094562,-1.3500498587716774,0.7531503687480621,0.6568705569879226,1.097616608426526,-1.5058562646303209,0.9978367626780669,0.0,0.0]    |\n",
      "|[636.0,32.0,8.0,0.0,2.0,1.0,0.0,138555.46,1.0,0.0,0.0]|[-0.13402604055474665,-0.8667990012785743,1.0216450537664428,-1.3500498587716774,0.7531503687480621,0.6568705569879226,-0.9109521897380475,0.6479773830792573,0.9978367626780669,0.0,0.0] |\n",
      "+------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "#  Normaliser les features avec StandardScaler      # Cr√©er le StandardScaler\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_raw\",\n",
    "    outputCol=\"features\",\n",
    "    withStd=True,  # Normaliser avec √©cart-type\n",
    "    withMean=True  # Centrer sur la moyenne\n",
    ")\n",
    "# Entra√Æner le scaler sur les donn√©es\n",
    "scaler_model = scaler.fit(df_avec_features)\n",
    "\n",
    "# Appliquer la normalisation\n",
    "df_final = scaler_model.transform(df_avec_features)\n",
    "\n",
    "# Afficher le r√©sultat\n",
    "print(\"Features normalis√©es (premiers exemples):\")\n",
    "df_final.select(\"features_raw\", \"features\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c591fb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# par convention en change le nom de exited to label\n",
    "df_final = df_final.withColumnRenamed(\"Exited\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "edc0b880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jeu d'entra√Ænement: 6535   \n",
      "jeu de test: 1546 \n",
      "Distribution dans train_data:\n",
      "+------+-----+\n",
      "|Exited|count|\n",
      "+------+-----+\n",
      "|   0.0| 3308|\n",
      "|   1.0| 3227|\n",
      "+------+-----+\n",
      "\n",
      "Distribution dans test_data:\n",
      "+------+-----+\n",
      "|Exited|count|\n",
      "+------+-----+\n",
      "|   0.0|  754|\n",
      "|   1.0|  792|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# S√©parer les donn√©es en train (80%) et test (20%)\n",
    "train_data, test_data = df_final.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"jeu d'entra√Ænement: {train_data.count()}   \" )\n",
    "print(f\"jeu de test: {test_data.count()} \")\n",
    "\n",
    "# V√©rifier la distribution dans chaque set\n",
    "print(\"Distribution dans train_data:\")\n",
    "train_data.groupBy(\"Exited\").count().show()\n",
    "\n",
    "print(\"Distribution dans test_data:\")\n",
    "test_data.groupBy(\"Exited\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c149ad86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âTAPE 6.8 : Choisir et cr√©er le mod√®le MLlib\n",
    "# Teste 3 mod√®les MLlib :\n",
    "from pyspark.ml.classification import (\n",
    "    LogisticRegression,\n",
    "    RandomForestClassifier,\n",
    "    GBTClassifier\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e7e5ca",
   "metadata": {},
   "source": [
    "## PIPLELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "331cbcc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "√âTAPE 6 : Construction du Pipeline ML\n",
      "============================================================\n",
      "‚úÖ Donn√©es d'entra√Ænement : 6535 lignes\n",
      "‚úÖ Donn√©es de test : 1546 lignes\n",
      "\n",
      "üìä Distribution dans train_data:\n",
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  0.0| 3308|\n",
      "|  1.0| 3227|\n",
      "+-----+-----+\n",
      "\n",
      "üìä Distribution dans test_data:\n",
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  0.0|  754|\n",
      "|  1.0|  792|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# PIPELINE \n",
    "# Pr√©diction de l'attrition bancaire\n",
    "# ============================================\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "# ============================================\n",
    "# √âTAPE 6 : CONSTRUCTION DU PIPELINE\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"√âTAPE 6 : Construction du Pipeline ML\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. D√©finir les colonnes features (d√©j√† fait dans votre code)\n",
    "colonnes_features = [\n",
    "    \"CreditScore\", \"Age\", \"Tenure\", \"Balance\", \"NumOfProducts\",\n",
    "    \"HasCrCard\", \"IsActiveMember\", \"EstimatedSalary\", \"GenderIndex\",\n",
    "    \"Geography_France\", \"Geography_Germany\"\n",
    "]\n",
    "\n",
    "# 2. Cr√©er les √©tapes du pipeline (transformations)\n",
    "# √âtape 1 : Assembler les features en un seul vecteur\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=colonnes_features,\n",
    "    outputCol=\"features_raw\"  # Vecteur brut avant normalisation\n",
    ")\n",
    "\n",
    "# √âtape 2 : Normaliser les features (StandardScaler)\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_raw\",   # Entr√©e : vecteur brut\n",
    "    outputCol=\"features\",       # Sortie : vecteur normalis√©\n",
    "    withStd=True,               # Diviser par l'√©cart-type\n",
    "    withMean=True               # Centrer sur la moyenne\n",
    ")\n",
    "\n",
    "# 3. Renommer la colonne cible en \"label\" (obligatoire pour MLlib)\n",
    "df_prepared = df_equilibre.withColumnRenamed(\"Exited\", \"label\")\n",
    "\n",
    "# 4. S√©parer les donn√©es : 80% train, 20% test\n",
    "train_data, test_data = df_prepared.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# IMPORTANT : Cacher les donn√©es car elles seront utilis√©es plusieurs fois\n",
    "train_data.cache()\n",
    "test_data.cache()\n",
    "\n",
    "print(f\"‚úÖ Donn√©es d'entra√Ænement : {train_data.count()} lignes\")\n",
    "print(f\"‚úÖ Donn√©es de test : {test_data.count()} lignes\")\n",
    "\n",
    "# V√©rifier la distribution des classes\n",
    "print(\"\\nüìä Distribution dans train_data:\")\n",
    "train_data.groupBy(\"label\").count().show()\n",
    "\n",
    "print(\"üìä Distribution dans test_data:\")\n",
    "test_data.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e992d0",
   "metadata": {},
   "source": [
    "## ENTRAINER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c66f24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "√âTAPE 7 : Entra√Ænement des Mod√®les\n",
      "============================================================\n",
      "\n",
      "üîπ Mod√®le : Logistic Regression\n",
      "‚è≥ Entra√Ænement en cours...\n",
      "‚úÖ Entra√Ænement termin√©!\n",
      "\n",
      "üîπ Mod√®le : Random Forest\n",
      "‚è≥ Entra√Ænement en cours...\n",
      "‚úÖ Entra√Ænement termin√©!\n",
      "\n",
      "üîπ Mod√®le : Gradient Boosted Trees\n",
      "‚è≥ Entra√Ænement en cours...\n",
      "‚úÖ Entra√Ænement termin√©!\n",
      "\n",
      "‚úÖ Tous les mod√®les ont √©t√© entra√Æn√©s avec succ√®s!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# ============================================\n",
    "# √âTAPE 7 : ENTRA√éNEMENT ET VALIDATION CROIS√âE\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"√âTAPE 7 : Entra√Ænement des Mod√®les\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10)\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", seed=42)\n",
    "gbt = GBTClassifier(featuresCol=\"features\", labelCol=\"label\", seed=42)\n",
    "\n",
    "# Liste des mod√®les avec leurs configurations\n",
    "modeles = [\n",
    "    {\n",
    "        \"nom\": \"Logistic Regression\",\n",
    "        \"modele\": LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10),\n",
    "        \"param_grid\": ParamGridBuilder()\n",
    "            .addGrid(LogisticRegression().regParam, [0.01, 0.1])  # ‚ö†Ô∏è Correction ici\n",
    "            .addGrid(LogisticRegression().elasticNetParam, [0.0, 0.5])\n",
    "            .build()\n",
    "    },\n",
    "    {\n",
    "        \"nom\": \"Random Forest\",\n",
    "        \"modele\": RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", seed=42),\n",
    "        \"param_grid\": ParamGridBuilder()\n",
    "            .addGrid(RandomForestClassifier().numTrees, [50, 100])  # ‚ö†Ô∏è Correction ici\n",
    "            .addGrid(RandomForestClassifier().maxDepth, [5, 10])\n",
    "            .build()\n",
    "    },\n",
    "    {\n",
    "        \"nom\": \"Gradient Boosted Trees\",\n",
    "        \"modele\": GBTClassifier(featuresCol=\"features\", labelCol=\"label\", seed=42),\n",
    "        \"param_grid\": ParamGridBuilder()\n",
    "            .addGrid(GBTClassifier().maxIter, [50, 100])  # ‚ö†Ô∏è Correction ici\n",
    "            .addGrid(GBTClassifier().maxDepth, [3, 5])\n",
    "            .build()\n",
    "    }\n",
    "]\n",
    "\n",
    "# √âvaluateur commun pour tous les mod√®les\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"  # M√©trique principale\n",
    ")\n",
    "\n",
    "# Dictionnaire pour stocker les mod√®les entra√Æn√©s\n",
    "modeles_entraines = {}  # ‚ö†Ô∏è Correction orthographe\n",
    "\n",
    "# Boucle d'entra√Ænement pour chaque mod√®le\n",
    "for m in modeles:\n",
    "    print(f\"\\nüîπ Mod√®le : {m['nom']}\")\n",
    "    \n",
    "    # Cr√©er le pipeline complet (assemblage + normalisation + mod√®le)\n",
    "    pipeline = Pipeline(stages=[assembler, scaler, m[\"modele\"]])\n",
    "    \n",
    "    # Cr√©er le CrossValidator (validation crois√©e)\n",
    "    cv = CrossValidator(\n",
    "        estimator=pipeline,\n",
    "        estimatorParamMaps=m[\"param_grid\"],\n",
    "        evaluator=evaluator,\n",
    "        numFolds=3,  # 3-fold cross validation\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    # Entra√Æner le mod√®le\n",
    "    print(f\"‚è≥ Entra√Ænement en cours...\")\n",
    "    modele_entraine = cv.fit(train_data)\n",
    "    print(\"‚úÖ Entra√Ænement termin√©!\")\n",
    "    \n",
    "    # Sauvegarder le mod√®le entra√Æn√©\n",
    "    modeles_entraines[m[\"nom\"]] = modele_entraine\n",
    "\n",
    "print(\"\\n‚úÖ Tous les mod√®les ont √©t√© entra√Æn√©s avec succ√®s!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "24790139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "√âTAPE 8 : √âvaluation des Mod√®les\n",
      "============================================================\n",
      "\n",
      "üìä √âvaluation : Logistic Regression\n",
      "--------------------------------------------------\n",
      "  AUC-ROC    : 0.7542\n",
      "  Accuracy   : 0.6889\n",
      "  Precision  : 0.6911\n",
      "  Recall     : 0.6889\n",
      "  F1-Score   : 0.6886\n",
      "\n",
      "  üìã Matrice de Confusion:\n",
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  0.0|       0.0|  548|\n",
      "|  0.0|       1.0|  206|\n",
      "|  1.0|       0.0|  275|\n",
      "|  1.0|       1.0|  517|\n",
      "+-----+----------+-----+\n",
      "\n",
      "\n",
      "üìä √âvaluation : Random Forest\n",
      "--------------------------------------------------\n",
      "  AUC-ROC    : 0.8470\n",
      "  Accuracy   : 0.7600\n",
      "  Precision  : 0.7644\n",
      "  Recall     : 0.7600\n",
      "  F1-Score   : 0.7595\n",
      "\n",
      "  üìã Matrice de Confusion:\n",
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  0.0|       0.0|  614|\n",
      "|  0.0|       1.0|  140|\n",
      "|  1.0|       0.0|  231|\n",
      "|  1.0|       1.0|  561|\n",
      "+-----+----------+-----+\n",
      "\n",
      "\n",
      "üìä √âvaluation : Gradient Boosted Trees\n",
      "--------------------------------------------------\n",
      "  AUC-ROC    : 0.8652\n",
      "  Accuracy   : 0.7691\n",
      "  Precision  : 0.7720\n",
      "  Recall     : 0.7691\n",
      "  F1-Score   : 0.7688\n",
      "\n",
      "  üìã Matrice de Confusion:\n",
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  0.0|       0.0|  612|\n",
      "|  0.0|       1.0|  142|\n",
      "|  1.0|       0.0|  215|\n",
      "|  1.0|       1.0|  577|\n",
      "+-----+----------+-----+\n",
      "\n",
      "\n",
      "============================================================\n",
      "üìà R√âCAPITULATIF DES PERFORMANCES\n",
      "============================================================\n",
      "\n",
      "Mod√®le                         AUC-ROC    Accuracy   F1-Score  \n",
      "------------------------------------------------------------\n",
      "Logistic Regression            0.7542     0.6889     0.6886    \n",
      "Random Forest                  0.8470     0.7600     0.7595    \n",
      "Gradient Boosted Trees         0.8652     0.7691     0.7688    \n",
      "\n",
      "============================================================\n",
      "üèÜ MEILLEUR MOD√àLE : Gradient Boosted Trees\n",
      "   AUC-ROC = 0.8652\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# √âTAPE 8 : √âVALUATION DES MOD√àLES\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"√âTAPE 8 : √âvaluation des Mod√®les\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Fonction pour √©valuer un mod√®le\n",
    "def evaluer_modele(nom_modele, modele, test_data):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        dict: Dictionnaire avec toutes les m√©triques\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìä √âvaluation : {nom_modele}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # 1. FAIRE LES PR√âDICTIONS sur le test set\n",
    "    predictions = modele.transform(test_data)\n",
    "    \n",
    "    # 2. CALCULER LES M√âTRIQUES\n",
    "    \n",
    "    # AUC-ROC (m√©trique principale pour classification binaire)\n",
    "    evaluator_auc = BinaryClassificationEvaluator(\n",
    "        labelCol=\"label\",\n",
    "        rawPredictionCol=\"rawPrediction\",\n",
    "        metricName=\"areaUnderROC\"\n",
    "    )\n",
    "    auc = evaluator_auc.evaluate(predictions)\n",
    "    \n",
    "    # Accuracy (Pr√©cision globale)\n",
    "    evaluator_acc = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"label\",\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=\"accuracy\"\n",
    "    )\n",
    "    accuracy = evaluator_acc.evaluate(predictions)\n",
    "    \n",
    "    # Precision (Pr√©cision par classe)\n",
    "    evaluator_precision = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"label\",\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=\"weightedPrecision\"\n",
    "    )\n",
    "    precision = evaluator_precision.evaluate(predictions)\n",
    "    \n",
    "    # Recall (Rappel)\n",
    "    evaluator_recall = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"label\",\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=\"weightedRecall\"\n",
    "    )\n",
    "    recall = evaluator_recall.evaluate(predictions)\n",
    "    \n",
    "    # F1-Score (Moyenne harmonique de Precision et Recall)\n",
    "    evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"label\",\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=\"f1\"\n",
    "    )\n",
    "    f1 = evaluator_f1.evaluate(predictions)\n",
    "    \n",
    "    # 3. AFFICHER LES R√âSULTATS\n",
    "    print(f\"  AUC-ROC    : {auc:.4f}\")\n",
    "    print(f\"  Accuracy   : {accuracy:.4f}\")\n",
    "    print(f\"  Precision  : {precision:.4f}\")\n",
    "    print(f\"  Recall     : {recall:.4f}\")\n",
    "    print(f\"  F1-Score   : {f1:.4f}\")\n",
    "    \n",
    "    # 4. MATRICE DE CONFUSION\n",
    "    print(\"\\n  üìã Matrice de Confusion:\")\n",
    "    confusion_matrix = predictions.groupBy(\"label\", \"prediction\").count()\n",
    "    confusion_matrix.orderBy(\"label\", \"prediction\").show()\n",
    "    \n",
    "    # Retourner les m√©triques\n",
    "    return {\n",
    "        \"nom\": nom_modele,\n",
    "        \"auc\": auc,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "# ---------------------------------------------\n",
    "# √âVALUER TOUS LES MOD√àLES\n",
    "# ---------------------------------------------\n",
    "\n",
    "resultats = []\n",
    "\n",
    "for nom_modele, modele in modeles_entraines.items():\n",
    "    metrics = evaluer_modele(nom_modele, modele, test_data)\n",
    "    resultats.append(metrics)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# COMPARAISON FINALE\n",
    "# ---------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìà R√âCAPITULATIF DES PERFORMANCES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Afficher un tableau comparatif\n",
    "print(f\"\\n{'Mod√®le':<30} {'AUC-ROC':<10} {'Accuracy':<10} {'F1-Score':<10}\")\n",
    "print(\"-\" * 60)\n",
    "for r in resultats:\n",
    "    print(f\"{r['nom']:<30} {r['auc']:<10.4f} {r['accuracy']:<10.4f} {r['f1']:<10.4f}\")\n",
    "\n",
    "# Trouver le meilleur mod√®le (bas√© sur AUC-ROC)\n",
    "meilleur = max(resultats, key=lambda x: x['auc'])\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"üèÜ MEILLEUR MOD√àLE : {meilleur['nom']}\")\n",
    "print(f\"   AUC-ROC = {meilleur['auc']:.4f}\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f01cb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "√âTAPE 9 : Sauvegarde du Mod√®le\n",
      "============================================================\n",
      "‚ùå Erreur lors de la sauvegarde : An error occurred while calling o57469.save.\n",
      ": java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://cwiki.apache.org/confluence/display/HADOOP2/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:789)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:298)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:314)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:1116)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:798)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:838)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:837)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:837)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:837)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:988)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:268)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:306)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:189)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:195)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:117)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:115)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:129)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)\n",
      "\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)\n",
      "\tat org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\n",
      "\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)\n",
      "\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.text(DataFrameWriter.scala:409)\n",
      "\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:445)\n",
      "\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$saveImpl$1(Pipeline.scala:264)\n",
      "\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$saveImpl$1$adapted(Pipeline.scala:261)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:226)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:226)\n",
      "\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.saveImpl(Pipeline.scala:261)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.saveImpl(Pipeline.scala:370)\n",
      "\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:169)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.super$save(Pipeline.scala:368)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$4(Pipeline.scala:368)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent(events.scala:174)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent$(events.scala:169)\n",
      "\tat org.apache.spark.ml.util.Instrumentation.withSaveInstanceEvent(Instrumentation.scala:44)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3(Pipeline.scala:368)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3$adapted(Pipeline.scala:368)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:226)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:226)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.save(Pipeline.scala:368)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n",
      "\t\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:789)\n",
      "\t\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:298)\n",
      "\t\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:314)\n",
      "\t\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:1116)\n",
      "\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:798)\n",
      "\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:838)\n",
      "\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\n",
      "\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:837)\n",
      "\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\n",
      "\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:837)\n",
      "\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\n",
      "\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:837)\n",
      "\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\n",
      "\t\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:988)\n",
      "\t\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\n",
      "\t\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:190)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:268)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:306)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:189)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:195)\n",
      "\t\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:117)\n",
      "\t\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:115)\n",
      "\t\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:129)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\n",
      "\t\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\t\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n",
      "\t\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n",
      "\t\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\n",
      "\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)\n",
      "\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\n",
      "\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\n",
      "\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)\n",
      "\t\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\t\t... 40 more\n",
      "Caused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://cwiki.apache.org/confluence/display/HADOOP2/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:601)\n",
      "\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:622)\n",
      "\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:645)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:742)\n",
      "\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1954)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1912)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1885)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$install$1(ShutdownHookManager.scala:194)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat scala.Option.fold(Option.scala:263)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:195)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:55)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:53)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:159)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala:63)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:250)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:99)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:379)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:961)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:204)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:227)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:96)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1132)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1141)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:521)\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:492)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:569)\n",
      "\t... 27 more\n",
      "\n",
      "\n",
      "üßπ Nettoyage de la m√©moire...\n",
      "\n",
      "============================================================\n",
      "‚úÖ PIPELINE TERMIN√â AVEC SUCC√àS!\n",
      "============================================================\n",
      "\n",
      "üìä R√©sum√© final :\n",
      "   - 3 mod√®les entra√Æn√©s\n",
      "   - Meilleur mod√®le : Gradient Boosted Trees\n",
      "   - AUC-ROC : 0.8652\n",
      "   - Mod√®le sauvegard√© : ../models/best_model_attrition\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# √âTAPE 9 : SAUVEGARDE DU MOD√àLE\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"√âTAPE 9 : Sauvegarde du Mod√®le\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# R√©cup√©rer le meilleur mod√®le\n",
    "best_model = modeles_entraines[meilleur['nom']]\n",
    "\n",
    "# Chemin de sauvegarde\n",
    "chemin_sauvegarde = \"../models/best_model_attrition\"\n",
    "\n",
    "# Sauvegarder le mod√®le\n",
    "best_model = best_model.bestModel\n",
    "try:\n",
    "    best_model.write().overwrite().save(chemin_sauvegarde)\n",
    "    print(f\"‚úÖ Mod√®le sauvegard√© dans : {chemin_sauvegarde}\")\n",
    "    print(f\"   Mod√®le : {meilleur['nom']}\")\n",
    "    print(f\"   Performance (AUC-ROC) : {meilleur['auc']:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur lors de la sauvegarde : {e}\")\n",
    "\n",
    "\n",
    "# Lib√©rer la m√©moire cache\n",
    "print(\"\\nüßπ Nettoyage de la m√©moire...\")\n",
    "train_data.unpersist()\n",
    "test_data.unpersist()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ PIPELINE TERMIN√â AVEC SUCC√àS!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüìä R√©sum√© final :\")\n",
    "print(f\"   - {len(modeles_entraines)} mod√®les entra√Æn√©s\")\n",
    "print(f\"   - Meilleur mod√®le : {meilleur['nom']}\")\n",
    "print(f\"   - AUC-ROC : {meilleur['auc']:.4f}\")\n",
    "print(f\"   - Mod√®le sauvegard√© : {chemin_sauvegarde}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "736b554e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "√âTAPE 9 : Sauvegarde du DataFrame final en CSV\n",
      "============================================================\n",
      "‚úÖ DataFrame sauvegard√© en CSV : ../models\\best_model_attrition.csv\n",
      "\n",
      "üßπ Nettoyage de la m√©moire...\n",
      "\n",
      "============================================================\n",
      "‚úÖ PIPELINE TERMIN√â AVEC SUCC√àS!\n",
      "============================================================\n",
      "\n",
      "üìä R√©sum√© final :\n",
      "   - 3 mod√®les entra√Æn√©s\n",
      "   - Meilleur mod√®le : Gradient Boosted Trees\n",
      "   - AUC-ROC : 0.8652\n",
      "   - DataFrame sauvegard√© en CSV : ../models\\best_model_attrition.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"√âTAPE 9 : Sauvegarde du DataFrame final en CSV\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Cr√©er le dossier ../models s'il n'existe pas\n",
    "chemin_dossier = \"../models\"\n",
    "os.makedirs(chemin_dossier, exist_ok=True)\n",
    "\n",
    "# Chemin complet du CSV\n",
    "chemin_csv = os.path.join(chemin_dossier, \"best_model_attrition.csv\")\n",
    "\n",
    "# Conversion en pandas et sauvegarde\n",
    "try:\n",
    "    df_final_pd = df_final.toPandas()\n",
    "    df_final_pd.to_csv(chemin_csv, index=False)\n",
    "    print(f\"‚úÖ DataFrame sauvegard√© en CSV : {chemin_csv}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur lors de la sauvegarde CSV : {e}\")\n",
    "\n",
    "# Nettoyage m√©moire\n",
    "print(\"\\nüßπ Nettoyage de la m√©moire...\")\n",
    "train_data.unpersist()\n",
    "test_data.unpersist()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ PIPELINE TERMIN√â AVEC SUCC√àS!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüìä R√©sum√© final :\")\n",
    "print(f\"   - {len(modeles_entraines)} mod√®les entra√Æn√©s\")  # v√©rifie le nom exact du dictionnaire\n",
    "print(f\"   - Meilleur mod√®le : {meilleur['nom']}\")\n",
    "print(f\"   - AUC-ROC : {meilleur['auc']:.4f}\")\n",
    "print(f\"   - DataFrame sauvegard√© en CSV : {chemin_csv}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
