{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98f62f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.14.0-py3-none-any.whl (239 kB)\n",
      "     ------------------------------------ 240.0/240.0 kB 918.9 kB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy<3,>=1.25.2 in c:\\users\\elkho\\onedrive\\desktop\\ia\\briefs\\breif6_predictiondelattritionclientbancaire\\venv\\lib\\site-packages (from imbalanced-learn) (2.3.4)\n",
      "Collecting scipy<2,>=1.11.4\n",
      "  Downloading scipy-1.16.3-cp311-cp311-win_amd64.whl (38.7 MB)\n",
      "     ---------------------------------------- 38.7/38.7 MB 9.1 MB/s eta 0:00:00\n",
      "Collecting scikit-learn<2,>=1.4.2\n",
      "  Downloading scikit_learn-1.7.2-cp311-cp311-win_amd64.whl (8.9 MB)\n",
      "     ---------------------------------------- 8.9/8.9 MB 10.2 MB/s eta 0:00:00\n",
      "Collecting joblib<2,>=1.2.0\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "     ------------------------------------- 308.4/308.4 kB 18.6 MB/s eta 0:00:00\n",
      "Collecting threadpoolctl<4,>=2.0.0\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn, imbalanced-learn\n",
      "Successfully installed imbalanced-learn-0.14.0 joblib-1.5.2 scikit-learn-1.7.2 scipy-1.16.3 threadpoolctl-3.6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "! pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "410b3dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aucune session Spark √† arr√™ter\n",
      "Python path: c:\\Users\\elkho\\OneDrive\\Desktop\\IA\\Briefs\\breif6_PredictiondelAttritionClientBancaire\\venv\\Scripts\\python.exe\n",
      "‚úì Nouvelle session Spark cr√©√©e\n",
      "‚úì Donn√©es r√©cup√©r√©es: 10000 lignes\n",
      "‚úì Donn√©es nettoy√©es: 10000 lignes\n",
      "‚úì Colonnes: ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary', 'Exited', 'GenderIndex', 'Geography_France', 'Geography_Germany']\n",
      "‚úì DataFrame Spark cr√©√© avec succ√®s: 10000 lignes\n",
      "\n",
      "‚úì Sch√©ma du DataFrame:\n",
      "root\n",
      " |-- CreditScore: long (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      " |-- Tenure: long (nullable = true)\n",
      " |-- Balance: double (nullable = true)\n",
      " |-- NumOfProducts: long (nullable = true)\n",
      " |-- HasCrCard: long (nullable = true)\n",
      " |-- IsActiveMember: long (nullable = true)\n",
      " |-- EstimatedSalary: double (nullable = true)\n",
      " |-- Exited: long (nullable = true)\n",
      " |-- GenderIndex: double (nullable = true)\n",
      " |-- Geography_France: double (nullable = true)\n",
      " |-- Geography_Germany: double (nullable = true)\n",
      "\n",
      "\n",
      "‚úì √âchantillon des donn√©es:\n",
      "+-----------+---+------+---------+-------------+---------+--------------+---------------+------+-----------+----------------+-----------------+\n",
      "|CreditScore|Age|Tenure|  Balance|NumOfProducts|HasCrCard|IsActiveMember|EstimatedSalary|Exited|GenderIndex|Geography_France|Geography_Germany|\n",
      "+-----------+---+------+---------+-------------+---------+--------------+---------------+------+-----------+----------------+-----------------+\n",
      "|        619| 42|     2|      0.0|            1|        1|             1|      101348.88|     1|        1.0|             1.0|              0.0|\n",
      "|        608| 41|     1| 83807.86|            1|        0|             1|      112542.58|     0|        1.0|             0.0|              0.0|\n",
      "|        502| 42|     8| 159660.8|            3|        1|             0|      113931.57|     1|        1.0|             1.0|              0.0|\n",
      "|        699| 39|     1|      0.0|            2|        0|             0|       93826.63|     0|        1.0|             1.0|              0.0|\n",
      "|        850| 43|     2|125510.82|            1|        1|             1|        79084.1|     0|        1.0|             0.0|              0.0|\n",
      "+-----------+---+------+---------+-------------+---------+--------------+---------------+------+-----------+----------------+-----------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# √âTAPE 1 : Arr√™ter compl√®tement Spark\n",
    "import os\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"‚úì Spark session arr√™t√©e\")\n",
    "except:\n",
    "    print(\"Aucune session Spark √† arr√™ter\")\n",
    "\n",
    "# Attendre un peu\n",
    "import time\n",
    "time.sleep(3)\n",
    "\n",
    "# √âTAPE 2 : Configuration des variables d'environnement\n",
    "# Trouver le chemin Python actuel\n",
    "python_path = sys.executable\n",
    "print(f\"Python path: {python_path}\")\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = python_path\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = python_path\n",
    "\n",
    "# √âTAPE 3 : Cr√©er une nouvelle session Spark avec configuration robuste\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Bank Churn Prediction\") \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\") \\\n",
    "    .config(\"spark.python.worker.reuse\", \"true\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"120s\") \\\n",
    "    .config(\"spark.network.timeout\", \"800s\") \\\n",
    "    .config(\"spark.rpc.askTimeout\", \"600s\") \\\n",
    "    .config(\"spark.core.connection.ack.wait.timeout\", \"600s\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"‚úì Nouvelle session Spark cr√©√©e\")\n",
    "\n",
    "# √âTAPE 4 : R√©cup√©rer et pr√©parer les donn√©es\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"clients_bancaires\"]\n",
    "collection = db[\"df_clean_collection\"]\n",
    "\n",
    "# R√©cup√©rer les donn√©es\n",
    "data = list(collection.find())\n",
    "df_pandas = pd.DataFrame(data)\n",
    "\n",
    "print(f\"‚úì Donn√©es r√©cup√©r√©es: {len(df_pandas)} lignes\")\n",
    "\n",
    "# Supprimer _id et colonnes non n√©cessaires\n",
    "columns_to_drop = ['_id', 'CustomerId', 'Surname']\n",
    "for col in columns_to_drop:\n",
    "    if col in df_pandas.columns:\n",
    "        df_pandas = df_pandas.drop(col, axis=1)\n",
    "\n",
    "# Traiter GeographyVec\n",
    "df_pandas['Geography_France'] = df_pandas['GeographyVec'].apply(lambda x: float(x[0]) if isinstance(x, list) and len(x) > 0 else 0.0)\n",
    "df_pandas['Geography_Germany'] = df_pandas['GeographyVec'].apply(lambda x: float(x[1]) if isinstance(x, list) and len(x) > 1 else 0.0)\n",
    "df_pandas = df_pandas.drop('GeographyVec', axis=1)\n",
    "\n",
    "# Convertir toutes les colonnes en float\n",
    "for col in df_pandas.columns:\n",
    "    if col not in ['Geography_France', 'Geography_Germany']:\n",
    "        df_pandas[col] = pd.to_numeric(df_pandas[col], errors='coerce')\n",
    "\n",
    "# Supprimer les lignes avec des valeurs nulles\n",
    "df_pandas = df_pandas.dropna()\n",
    "\n",
    "print(f\"‚úì Donn√©es nettoy√©es: {len(df_pandas)} lignes\")\n",
    "print(f\"‚úì Colonnes: {list(df_pandas.columns)}\")\n",
    "\n",
    "# √âTAPE 5 : Cr√©er le DataFrame Spark avec gestion d'erreur\n",
    "try:\n",
    "    # Convertir en Spark DataFrame\n",
    "    df_spark = spark.createDataFrame(df_pandas)\n",
    "    \n",
    "    # Forcer l'action (cache pour √©viter les recalculs)\n",
    "    df_spark = df_spark.cache()\n",
    "    \n",
    "    # Compter avec try-except\n",
    "    count = df_spark.count()\n",
    "    print(f\"‚úì DataFrame Spark cr√©√© avec succ√®s: {count} lignes\")\n",
    "    \n",
    "    # Afficher le sch√©ma\n",
    "    print(\"\\n‚úì Sch√©ma du DataFrame:\")\n",
    "    df_spark.printSchema()\n",
    "    \n",
    "    # Afficher un √©chantillon\n",
    "    print(\"\\n‚úì √âchantillon des donn√©es:\")\n",
    "    df_spark.show(5)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur lors de la cr√©ation du DataFrame Spark: {e}\")\n",
    "    print(\"\\n‚ö†Ô∏è Utilisation de Pandas/Scikit-learn √† la place...\")\n",
    "    \n",
    "    # PLAN B : Utiliser directement Pandas et scikit-learn\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, accuracy_score, f1_score\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PIPELINE AVEC SCIKIT-LEARN\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Pr√©parer X et y\n",
    "    feature_columns = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', \n",
    "                       'HasCrCard', 'IsActiveMember', 'EstimatedSalary', 'GenderIndex',\n",
    "                       'Geography_France', 'Geography_Germany']\n",
    "    \n",
    "    X = df_pandas[feature_columns]\n",
    "    y = df_pandas['Exited'].astype(int)\n",
    "    \n",
    "    print(f\"\\n‚úì Features: {feature_columns}\")\n",
    "    print(f\"‚úì Target distribution:\\n{y.value_counts()}\")\n",
    "    \n",
    "    # Gestion du d√©s√©quilibre\n",
    "    rus = RandomUnderSampler(random_state=42)\n",
    "    X_balanced, y_balanced = rus.fit_resample(X, y)\n",
    "    \n",
    "    print(f\"\\n‚úì Apr√®s undersampling:\\n{pd.Series(y_balanced).value_counts()}\")\n",
    "    \n",
    "    # Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_balanced, y_balanced, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Normalisation\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    print(f\"\\n‚úì Train: {len(X_train)} | Test: {len(X_test)}\")\n",
    "    \n",
    "    # Entra√Æner plusieurs mod√®les\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ENTRA√éNEMENT ET √âVALUATION DES MOD√àLES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n{name}:\")\n",
    "        \n",
    "        # Entra√Ænement\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Pr√©dictions\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "        \n",
    "        # M√©triques\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        \n",
    "        results[name] = {'accuracy': acc, 'f1': f1, 'auc': auc}\n",
    "        \n",
    "        print(f\"  - Accuracy: {acc:.4f}\")\n",
    "        print(f\"  - F1-Score: {f1:.4f}\")\n",
    "        print(f\"  - AUC-ROC: {auc:.4f}\")\n",
    "        \n",
    "        # Matrice de confusion\n",
    "        print(f\"\\n  Confusion Matrix:\")\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    # Meilleur mod√®le\n",
    "    best_model_name = max(results, key=lambda x: results[x]['auc'])\n",
    "    print(f\"\\nüèÜ Meilleur mod√®le: {best_model_name}\")\n",
    "    print(f\"   AUC-ROC: {results[best_model_name]['auc']:.4f}\")\n",
    "    \n",
    "    # Feature importance (si Random Forest ou Gradient Boosting)\n",
    "    if best_model_name in ['Random Forest', 'Gradient Boosting']:\n",
    "        best_model = models[best_model_name]\n",
    "        importances = best_model.feature_importances_\n",
    "        \n",
    "        print(f\"\\n‚úì Feature Importance ({best_model_name}):\")\n",
    "        for feature, importance in sorted(zip(feature_columns, importances), \n",
    "                                         key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  {feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "175ad4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark==3.5.1\n",
      "  Using cached pyspark-3.5.1.tar.gz (317.0 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting py4j==0.10.9.7\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Installing collected packages: py4j, pyspark\n",
      "  Attempting uninstall: py4j\n",
      "    Found existing installation: py4j 0.10.9.9\n",
      "    Uninstalling py4j-0.10.9.9:\n",
      "      Successfully uninstalled py4j-0.10.9.9\n",
      "  Attempting uninstall: pyspark\n",
      "    Found existing installation: pyspark 4.0.1\n",
      "    Uninstalling pyspark-4.0.1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 32] Le processus ne peut pas acc√©der au fichier car ce fichier est utilis√© par un autre processus: 'c:\\\\users\\\\elkho\\\\onedrive\\\\desktop\\\\ia\\\\briefs\\\\breif6_predictiondelattritionclientbancaire\\\\venv\\\\lib\\\\site-packages\\\\pyspark\\\\jars\\\\aircompressor-2.0.2.jar'\n",
      "Check the permissions.\n",
      "\n",
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "! pip install pyspark==3.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "beef2f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo in c:\\users\\elkho\\onedrive\\desktop\\ia\\briefs\\breif6_predictiondelattritionclientbancaire\\venv\\lib\\site-packages (4.15.3)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in c:\\users\\elkho\\onedrive\\desktop\\ia\\briefs\\breif6_predictiondelattritionclientbancaire\\venv\\lib\\site-packages (from pymongo) (2.8.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~yspark (C:\\Users\\elkho\\OneDrive\\Desktop\\IA\\Briefs\\breif6_PredictiondelAttritionClientBancaire\\venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~yspark (C:\\Users\\elkho\\OneDrive\\Desktop\\IA\\Briefs\\breif6_PredictiondelAttritionClientBancaire\\venv\\Lib\\site-packages)\n",
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\elkho\\onedrive\\desktop\\ia\\briefs\\breif6_predictiondelattritionclientbancaire\\venv\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\elkho\\onedrive\\desktop\\ia\\briefs\\breif6_predictiondelattritionclientbancaire\\venv\\lib\\site-packages (from pandas) (2.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\elkho\\onedrive\\desktop\\ia\\briefs\\breif6_predictiondelattritionclientbancaire\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\elkho\\onedrive\\desktop\\ia\\briefs\\breif6_predictiondelattritionclientbancaire\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\elkho\\onedrive\\desktop\\ia\\briefs\\breif6_predictiondelattritionclientbancaire\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\elkho\\onedrive\\desktop\\ia\\briefs\\breif6_predictiondelattritionclientbancaire\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~yspark (C:\\Users\\elkho\\OneDrive\\Desktop\\IA\\Briefs\\breif6_PredictiondelAttritionClientBancaire\\venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~yspark (C:\\Users\\elkho\\OneDrive\\Desktop\\IA\\Briefs\\breif6_PredictiondelAttritionClientBancaire\\venv\\Lib\\site-packages)\n",
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\elkho\\onedrive\\desktop\\ia\\briefs\\breif6_predictiondelattritionclientbancaire\\venv\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\elkho\\onedrive\\desktop\\ia\\briefs\\breif6_predictiondelattritionclientbancaire\\venv\\lib\\site-packages (from seaborn) (2.3.4)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\elkho\\onedrive\\desktop\\ia\\briefs\\breif6_predictiondelattritionclientbancaire\\venv\\lib\\site-packages (from seaborn) (2.3.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\elkho\\onedrive\\desktop\\ia\\briefs\\breif6_predictiondelattritionclientbancaire\\venv\\lib\\site-packages (from seaborn) (3.10.7)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\elkho\\onedrive\\desktop\\ia\\briefs\\breif6_predictiondelattritionclientbancaire\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\elkho\\onedrive\\desktop\\ia\\briefs\\breif6_predictiondelattritionclientbancaire\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\elkho\\onedrive\\desktop\\ia\\briefs\\breif6_predictiondelattritionclientbancaire\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\elkho\\onedrive\\desktop\\ia\\briefs\\breif6_predictiondelattritionclientbancaire\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\elkho\\onedrive\\desktop\\ia\\briefs\\breif6_predictiondelattritionclientbancaire\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\elkho\\onedrive\\desktop\\ia\\briefs\\breif6_predictiondelattritionclientbancaire\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\elkho\\onedrive\\desktop\\ia\\briefs\\breif6_predictiondelattritionclientbancaire\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\elkho\\onedrive\\desktop\\ia\\briefs\\breif6_predictiondelattritionclientbancaire\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\elkho\\onedrive\\desktop\\ia\\briefs\\breif6_predictiondelattritionclientbancaire\\venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\elkho\\onedrive\\desktop\\ia\\briefs\\breif6_predictiondelattritionclientbancaire\\venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\elkho\\onedrive\\desktop\\ia\\briefs\\breif6_predictiondelattritionclientbancaire\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~yspark (C:\\Users\\elkho\\OneDrive\\Desktop\\IA\\Briefs\\breif6_PredictiondelAttritionClientBancaire\\venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~yspark (C:\\Users\\elkho\\OneDrive\\Desktop\\IA\\Briefs\\breif6_PredictiondelAttritionClientBancaire\\venv\\Lib\\site-packages)\n",
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "! pip install pymongo\n",
    "! pip install pandas\n",
    "! pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cdc35cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session created successfully\n",
      "Version de Spark : 4.0.1\n",
      "Donn√©es charg√©es depuis MongoDB:\n",
      "+----------+--------+-----------+---+------+---------+-------------+---------+--------------+---------------+------+------------+-----------+\n",
      "|CustomerId| Surname|CreditScore|Age|Tenure|  Balance|NumOfProducts|HasCrCard|IsActiveMember|EstimatedSalary|Exited|GeographyVec|GenderIndex|\n",
      "+----------+--------+-----------+---+------+---------+-------------+---------+--------------+---------------+------+------------+-----------+\n",
      "|  15634602|Hargrave|        619| 42|     2|        0|            1|        1|             1|      101348.88|     1|  [1.0, 0.0]|        1.0|\n",
      "|  15647311|    Hill|        608| 41|     1| 83807.86|            1|        0|             1|      112542.58|     0|  [0.0, 0.0]|        1.0|\n",
      "|  15619304|    Onio|        502| 42|     8| 159660.8|            3|        1|             0|      113931.57|     1|  [1.0, 0.0]|        1.0|\n",
      "|  15701354|    Boni|        699| 39|     1|        0|            2|        0|             0|       93826.63|     0|  [1.0, 0.0]|        1.0|\n",
      "|  15737888|Mitchell|        850| 43|     2|125510.82|            1|        1|             1|        79084.1|     0|  [0.0, 0.0]|        1.0|\n",
      "+----------+--------+-----------+---+------+---------+-------------+---------+--------------+---------------+------+------------+-----------+\n",
      "only showing top 5 rows\n",
      "Sch√©ma des donn√©es:\n",
      "root\n",
      " |-- CustomerId: string (nullable = true)\n",
      " |-- Surname: string (nullable = true)\n",
      " |-- CreditScore: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Tenure: string (nullable = true)\n",
      " |-- Balance: string (nullable = true)\n",
      " |-- NumOfProducts: string (nullable = true)\n",
      " |-- HasCrCard: string (nullable = true)\n",
      " |-- IsActiveMember: string (nullable = true)\n",
      " |-- EstimatedSalary: string (nullable = true)\n",
      " |-- Exited: string (nullable = true)\n",
      " |-- GeographyVec: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- GenderIndex: double (nullable = true)\n",
      "\n",
      "Nombre de clients avec Exited=1 (churn): 2037\n",
      "Nombre de clients avec Exited=0 (pas de churn): 7963\n",
      "Distribution des classes:\n",
      "+------+-----+\n",
      "|Exited|count|\n",
      "+------+-----+\n",
      "|     1| 2037|\n",
      "|     0| 7963|\n",
      "+------+-----+\n",
      "\n",
      "Apr√®s √©quilibrage:\n",
      "+------+-----+\n",
      "|Exited|count|\n",
      "+------+-----+\n",
      "|     1| 2037|\n",
      "|     0| 2119|\n",
      "+------+-----+\n",
      "\n",
      "V√©rification des colonnes:\n",
      "‚úì CreditScore existe\n",
      "‚úì Age existe\n",
      "‚úì Tenure existe\n",
      "‚úì Balance existe\n",
      "‚úì NumOfProducts existe\n",
      "‚úì HasCrCard existe\n",
      "‚úì IsActiveMember existe\n",
      "‚úì EstimatedSalary existe\n",
      "‚úó gender_indexed MANQUANTE!\n",
      "‚úó geography_indexed MANQUANTE!\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "[FIELD_NOT_FOUND] No such struct field `gender_indexed` in `CustomerId`, `Surname`, `CreditScore`, `Age`, `Tenure`, `Balance`, `NumOfProducts`, `HasCrCard`, `IsActiveMember`, `EstimatedSalary`, `Exited`, `GeographyVec`, `GenderIndex`. SQLSTATE: 42704",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIllegalArgumentException\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 137\u001b[39m\n\u001b[32m    131\u001b[39m assembler = VectorAssembler(\n\u001b[32m    132\u001b[39m     inputCols=colonnes_features,\n\u001b[32m    133\u001b[39m     outputCol=\u001b[33m\"\u001b[39m\u001b[33mfeatures_raw\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    134\u001b[39m )\n\u001b[32m    136\u001b[39m \u001b[38;5;66;03m# Appliquer l'assembler\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m df_avec_features = \u001b[43massembler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_equilibre\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[38;5;66;03m# Afficher le r√©sultat\u001b[39;00m\n\u001b[32m    140\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFeatures assembl√©es (premiers exemples):\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elkho\\OneDrive\\Desktop\\IA\\Briefs\\breif6_PredictiondelAttritionClientBancaire\\venv\\Lib\\site-packages\\pyspark\\ml\\base.py:260\u001b[39m, in \u001b[36mTransformer.transform\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    258\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy(params)._transform(dataset)\n\u001b[32m    259\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    262\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m % \u001b[38;5;28mtype\u001b[39m(params))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elkho\\OneDrive\\Desktop\\IA\\Briefs\\breif6_PredictiondelAttritionClientBancaire\\venv\\Lib\\site-packages\\pyspark\\ml\\util.py:212\u001b[39m, in \u001b[36mtry_remote_transform_relation.<locals>.wrapped\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    210\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsupported \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elkho\\OneDrive\\Desktop\\IA\\Briefs\\breif6_PredictiondelAttritionClientBancaire\\venv\\Lib\\site-packages\\pyspark\\ml\\wrapper.py:429\u001b[39m, in \u001b[36mJavaTransformer._transform\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    428\u001b[39m \u001b[38;5;28mself\u001b[39m._transfer_params_to_java()\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_java_obj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m, dataset.sparkSession)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elkho\\OneDrive\\Desktop\\IA\\Briefs\\breif6_PredictiondelAttritionClientBancaire\\venv\\Lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[39m, in \u001b[36m__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gateway_client.gateway_property.enable_memory_management:\n\u001b[32m   1357\u001b[39m     value = weakref.ref(\n\u001b[32m   1358\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1359\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m wr, cc=\u001b[38;5;28mself\u001b[39m._gateway_client, \u001b[38;5;28mid\u001b[39m=\u001b[38;5;28mself\u001b[39m._target_id:\n\u001b[32m   1360\u001b[39m         _garbage_collect_object \u001b[38;5;129;01mand\u001b[39;00m _garbage_collect_object(cc, \u001b[38;5;28mid\u001b[39m))\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m     ThreadSafeFinalizer.add_finalizer(key, value)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elkho\\OneDrive\\Desktop\\IA\\Briefs\\breif6_PredictiondelAttritionClientBancaire\\venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:288\u001b[39m, in \u001b[36mdeco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n",
      "\u001b[31mIllegalArgumentException\u001b[39m: [FIELD_NOT_FOUND] No such struct field `gender_indexed` in `CustomerId`, `Surname`, `CreditScore`, `Age`, `Tenure`, `Balance`, `NumOfProducts`, `HasCrCard`, `IsActiveMember`, `EstimatedSalary`, `Exited`, `GeographyVec`, `GenderIndex`. SQLSTATE: 42704"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# √âTAPE 6 : CONSTRUCTION DU PIPELINE ML\n",
    "# ============================================\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from seaborn import boxplot\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"Prediction\")\n",
    "        .master(\"local[*]\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "print(\"Spark Session created successfully\")\n",
    "print(\"Version de Spark :\", spark.version)\n",
    "\n",
    "# ============================================\n",
    "# √âTAPE 6.1 : R√©cup√©rer les donn√©es depuis MongoDB\n",
    "# ============================================\n",
    "\n",
    "# Connexion √† MongoDB\n",
    "# client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "# db = client[\"bank_churn_db\"]\n",
    "# collection = db[\"preprocessed_data\"]\n",
    "\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"clients_bancaires\"]\n",
    "collection = db[\"df_clean_collection\"]\n",
    "\n",
    "# R√©cup√©rer toutes les donn√©es\n",
    "donnees_mongo = list(collection.find({}))\n",
    "\n",
    "# Convertir en DataFrame Pandas\n",
    "df_pandas = pd.DataFrame(donnees_mongo)\n",
    "# ‚úÖ SOLUTION : Supprimer la colonne _id AVANT conversion Spark\n",
    "if '_id' in df_pandas.columns:\n",
    "    df_pandas = df_pandas.drop('_id', axis=1)\n",
    "# Convertir en DataFrame Spark\n",
    "df = spark.createDataFrame(df_pandas)\n",
    "\n",
    "# Afficher les premi√®res lignes\n",
    "print(\"Donn√©es charg√©es depuis MongoDB:\")\n",
    "df.show(5)\n",
    "\n",
    "# Afficher le sch√©ma\n",
    "print(\"Sch√©ma des donn√©es:\")\n",
    "df.printSchema()\n",
    "\n",
    "# Fermer la connexion MongoDB\n",
    "client.close()\n",
    "\n",
    "# ============================================\n",
    "# √âTAPE 6.2 : V√©rifier et g√©rer le d√©s√©quilibre de classes\n",
    "# ============================================\n",
    "\n",
    "# Compter combien de clients ont Exited=1 et Exited=0\n",
    "nombre_exited_1 = df.filter(col(\"Exited\") == 1).count()\n",
    "nombre_exited_0 = df.filter(col(\"Exited\") == 0).count()\n",
    "\n",
    "print(f\"Nombre de clients avec Exited=1 (churn): {nombre_exited_1}\")\n",
    "print(f\"Nombre de clients avec Exited=0 (pas de churn): {nombre_exited_0}\")\n",
    "\n",
    "# Afficher la distribution\n",
    "print(\"Distribution des classes:\")\n",
    "df.groupBy(\"Exited\").count().show()\n",
    "\n",
    "# Si les classes sont tr√®s d√©s√©quilibr√©es, faire undersampling\n",
    "if nombre_exited_1 < nombre_exited_0:\n",
    "    # La classe 1 est minoritaire\n",
    "    nombre_a_garder = nombre_exited_1\n",
    "    df_classe_1 = df.filter(col(\"Exited\") == 1)\n",
    "    df_classe_0 = df.filter(col(\"Exited\") == 0)\n",
    "    # √âchantillonner la classe 0 pour avoir le m√™me nombre\n",
    "    df_classe_0_echantillon = df_classe_0.sample(False, nombre_a_garder / nombre_exited_0, seed=42)\n",
    "    df_equilibre = df_classe_1.union(df_classe_0_echantillon)\n",
    "else:\n",
    "    # La classe 0 est minoritaire\n",
    "    nombre_a_garder = nombre_exited_0\n",
    "    df_classe_1 = df.filter(col(\"Exited\") == 1)\n",
    "    df_classe_0 = df.filter(col(\"Exited\") == 0)\n",
    "    # √âchantillonner la classe 1 pour avoir le m√™me nombre\n",
    "    df_classe_1_echantillon = df_classe_1.sample(False, nombre_a_garder / nombre_exited_1, seed=42)\n",
    "    df_equilibre = df_classe_0.union(df_classe_1_echantillon)\n",
    "\n",
    "print(\"Apr√®s √©quilibrage:\")\n",
    "df_equilibre.groupBy(\"Exited\").count().show()\n",
    "\n",
    "# ============================================\n",
    "# √âTAPE 6.3 : D√©finir les colonnes features\n",
    "# ============================================\n",
    "\n",
    "# Toutes les colonnes num√©riques (features)\n",
    "colonnes_features = [\n",
    "    \"CreditScore\",\n",
    "    \"Age\",\n",
    "    \"Tenure\",\n",
    "    \"Balance\",\n",
    "    \"NumOfProducts\",\n",
    "    \"HasCrCard\",\n",
    "    \"IsActiveMember\",\n",
    "    \"EstimatedSalary\",\n",
    "    \"gender_indexed\",      # D√©j√† encod√©e\n",
    "    \"geography_indexed\"    # D√©j√† encod√©e\n",
    "]\n",
    "\n",
    "# Colonne cible\n",
    "colonne_cible = \"Exited\"\n",
    "\n",
    "# V√©rifier que toutes les colonnes existent\n",
    "print(\"V√©rification des colonnes:\")\n",
    "for col_name in colonnes_features:\n",
    "    if col_name in df_equilibre.columns:\n",
    "        print(f\"‚úì {col_name} existe\")\n",
    "    else:\n",
    "        print(f\"‚úó {col_name} MANQUANTE!\")\n",
    "\n",
    "# ============================================\n",
    "# √âTAPE 6.4 : Assembler toutes les features dans un vecteur\n",
    "# ============================================\n",
    "\n",
    "# Cr√©er le VectorAssembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=colonnes_features,\n",
    "    outputCol=\"features_raw\"\n",
    ")\n",
    "\n",
    "# Appliquer l'assembler\n",
    "df_avec_features = assembler.transform(df_equilibre)\n",
    "\n",
    "# Afficher le r√©sultat\n",
    "print(\"Features assembl√©es (premiers exemples):\")\n",
    "df_avec_features.select(colonnes_features + [\"features_raw\"]).show(5, truncate=False)\n",
    "\n",
    "# ============================================\n",
    "# √âTAPE 6.5 : Normaliser les features avec StandardScaler\n",
    "# ============================================\n",
    "\n",
    "# Cr√©er le StandardScaler\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_raw\",\n",
    "    outputCol=\"features\",\n",
    "    withStd=True,  # Normaliser avec √©cart-type\n",
    "    withMean=True  # Centrer sur la moyenne\n",
    ")\n",
    "\n",
    "# Entra√Æner le scaler sur les donn√©es\n",
    "scaler_model = scaler.fit(df_avec_features)\n",
    "\n",
    "# Appliquer la normalisation\n",
    "df_final = scaler_model.transform(df_avec_features)\n",
    "\n",
    "# Afficher le r√©sultat\n",
    "print(\"Features normalis√©es (premiers exemples):\")\n",
    "df_final.select(\"features_raw\", \"features\").show(5, truncate=False)\n",
    "\n",
    "# ============================================\n",
    "# √âTAPE 6.6 : Encoder la colonne cible (Exited) en \"label\"\n",
    "# ============================================\n",
    "\n",
    "# Cr√©er le StringIndexer pour la colonne cible\n",
    "indexer_target = StringIndexer(inputCol=\"Exited\", outputCol=\"label\")\n",
    "\n",
    "# Appliquer l'encodage\n",
    "df_final = indexer_target.fit(df_final).transform(df_final)\n",
    "\n",
    "# Afficher\n",
    "print(\"Label encod√© (Exited -> label):\")\n",
    "df_final.select(\"Exited\", \"label\").show(10)\n",
    "\n",
    "# ============================================\n",
    "# √âTAPE 6.7 : S√©parer les donn√©es en train (80%) et test (20%)\n",
    "# ============================================\n",
    "\n",
    "# S√©paration al√©atoire avec seed=42 pour reproductibilit√©\n",
    "train_data, test_data = df_equilibre.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Nombre d'exemples d'entra√Ænement: {train_data.count()}\")\n",
    "print(f\"Nombre d'exemples de test: {test_data.count()}\")\n",
    "\n",
    "# V√©rifier la distribution dans chaque set\n",
    "print(\"Distribution dans train_data:\")\n",
    "train_data.groupBy(\"Exited\").count().show()\n",
    "\n",
    "print(\"Distribution dans test_data:\")\n",
    "test_data.groupBy(\"Exited\").count().show()\n",
    "\n",
    "# ============================================\n",
    "# √âTAPE 6.8 : Choisir et cr√©er le mod√®le MLlib\n",
    "# ============================================\n",
    "\n",
    "# Random Forest Classifier (bon choix pour d√©butants)\n",
    "modele = RandomForestClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    numTrees=100,      # Nombre d'arbres dans la for√™t\n",
    "    maxDepth=10,       # Profondeur maximale des arbres\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"Mod√®le choisi: Random Forest Classifier\")\n",
    "\n",
    "# ============================================\n",
    "# √âTAPE 6.9 : Construire le Pipeline complet\n",
    "# ============================================\n",
    "\n",
    "# Cr√©er le pipeline avec toutes les √©tapes\n",
    "pipeline = Pipeline(stages=[\n",
    "    assembler,          # Assembler les features\n",
    "    scaler,             # Normaliser\n",
    "    indexer_target,     # Encoder la cible\n",
    "    modele              # Entra√Æner le mod√®le\n",
    "])\n",
    "\n",
    "print(\"Pipeline cr√©√© avec les √©tapes:\")\n",
    "print(\"1. VectorAssembler\")\n",
    "print(\"2. StandardScaler\")\n",
    "print(\"3. StringIndexer (target)\")\n",
    "print(\"4. RandomForestClassifier\")\n",
    "\n",
    "# ============================================\n",
    "# √âTAPE 6.10 : Entra√Æner le mod√®le\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"D√©but de l'entra√Ænement du mod√®le...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "pipeline_model = pipeline.fit(train_data)\n",
    "\n",
    "print(\"‚úì Entra√Ænement termin√©!\")\n",
    "\n",
    "# ============================================\n",
    "# √âTAPE 6.11 : Faire des pr√©dictions sur les donn√©es de test\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nFaire des pr√©dictions sur les donn√©es de test...\")\n",
    "predictions = pipeline_model.transform(test_data)\n",
    "\n",
    "# Afficher les pr√©dictions\n",
    "print(\"\\nExemples de pr√©dictions:\")\n",
    "predictions.select(\"Exited\", \"label\", \"prediction\", \"probability\").show(20, truncate=False)\n",
    "\n",
    "# ============================================\n",
    "# √âTAPE 6.12 : √âvaluer les performances du mod√®le\n",
    "# ============================================\n",
    "\n",
    "# √âvaluateur pour l'AUC (Area Under Curve) - m√©trique principale\n",
    "evaluator_auc = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "auc = evaluator_auc.evaluate(predictions)\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"AUC (Area Under ROC): {auc:.4f}\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "# √âvaluateur pour la pr√©cision (accuracy)\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "accuracy = evaluator_accuracy.evaluate(predictions)\n",
    "print(f\"Accuracy (Pr√©cision): {accuracy:.4f}\")\n",
    "\n",
    "# √âvaluateur pour la pr√©cision par classe (precision)\n",
    "evaluator_precision = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"weightedPrecision\"\n",
    ")\n",
    "\n",
    "precision = evaluator_precision.evaluate(predictions)\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "\n",
    "# √âvaluateur pour le rappel (recall)\n",
    "evaluator_recall = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"weightedRecall\"\n",
    ")\n",
    "\n",
    "recall = evaluator_recall.evaluate(predictions)\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "\n",
    "# ============================================\n",
    "# √âTAPE 6.13 : Afficher la matrice de confusion (simple)\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nMatrice de confusion:\")\n",
    "predictions.groupBy(\"label\", \"prediction\").count().orderBy(\"label\", \"prediction\").show()\n",
    "\n",
    "# ============================================\n",
    "# √âTAPE 6.14 : Sauvegarder le mod√®le entra√Æn√©\n",
    "# ============================================\n",
    "\n",
    "# Sauvegarder le pipeline complet\n",
    "chemin_modele = \"models/churn_prediction_model\"\n",
    "pipeline_model.write().overwrite().save(chemin_modele)\n",
    "\n",
    "print(f\"\\n‚úì Mod√®le sauvegard√© dans '{chemin_modele}'\")\n",
    "\n",
    "# Pour charger plus tard, utilisez :\n",
    "# from pyspark.ml import PipelineModel\n",
    "# pipeline_model_charge = PipelineModel.load(\"models/churn_prediction_model\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PIPELINE TERMIN√â AVEC SUCC√àS!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffe4f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.13.7\n",
      "<module 'pyspark.version' from 'c:\\\\Users\\\\elkho\\\\OneDrive\\\\Desktop\\\\IA\\\\Briefs\\\\breif6_PredictiondelAttritionClientBancaire\\\\venv\\\\Lib\\\\site-packages\\\\pyspark\\\\version.py'>\n"
     ]
    }
   ],
   "source": [
    "import platform, pyspark\n",
    "print(platform.python_version())\n",
    "print(pyspark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7747b3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
